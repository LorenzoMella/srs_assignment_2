---
title: "Statistical Research Skills -- Assignment 2"
author: "Lorenzo Mella (UUN: s1566023)"
output: pdf_document
---


```{r}
library(tidyverse, quietly = TRUE)
library(patchwork, quietly = TRUE)
```

# Description of the problem

Kernel density estimation (KDE) models the distribution under a criterion of smoothness of the density. Under the heuristic principle that datapoints in a sample generated form a distribution with density $f(x)$ tautologically accumulate with high probability in high-density regions, we associate to each datapoint $x_i$ a scaled kernel $K_h(x-x_i)$. The average of these functions hopefully provides a good approximation to the original density.

The choice of kernel is dictated by numerical requirements: if the kernel has unbounded support, for instance, every summand must be computed in $x$, to evaluate the density $\hat{f}(x)$.

It is also dictated by smoothness considerations: if we expect the data to be generate from a smooth density, it would be more sensible to use graded kernels such as a Gaussian or the Epanechnikov kernel. A rectangular kernel would be more appropriate for piecewise constant densities.


## Kernel density estimator

Problematic choices:

- bandwidth
- kernel shape


## Histogram estimator

Problematic choices:

- number of bins

## Mixture model estimator

Problematic parameter:

- conditional distributions
- number of conditional distributions


# Experimental setting

## Densities that represent a wide testing range of possibilities

```{r}
# Define densities and RNGs for the distributions to be tested

# Normal (unimodal base case)
ddistrib_1 = function(x) dnorm(x, 0, 3)
rdistrib_1 = function(n) rnorm(n, 0, 3)
# Student t (unimodal with heavier tails)
ddistrib_2 = function(x) dt(x / 3, df = 1)
rdistrib_2 = function(n) 3 * rt(n, df = 1)
# Uniform (discontinuous density with a dense modes)
ddistrib_3 = function(x) dunif(x, 0, 3)
rdistrib_3 = function(n) runif(n, 0, 3)
# Gamma (unimodal, positively skewed)
ddistrib_4 = function(x) dgamma(x, 3, 1)
rdistrib_4 = function(n) rgamma(n, 3, 1)
# Mixture of distributions
# (trimodal, with many challenging features)
ddistrib_5 = function(x) {
  0.5 * dnorm(x, 0, 1) + 0.3 * dgamma(8 - x, 2, 1.5) + 0.2 * dunif(x, 3, 4.5)
}
rdistrib_5 = function(n) {
  num_samples = rmultinom(1, n, c(0.5, 0.3, 0.2))
  sample(c(rnorm(num_samples[1], 0, 1),
           8 - rgamma(num_samples[2], 2, 1.5),
           runif(num_samples[3], 3, 4.5)))
}
```

## One shot experiments

- Graphs
- Discussion

```{r}
# A density function obtained from histogram data returned by hist

hist_density = function(hist_data, x) {
  # Find the bin where x lies: its index is the maximum number
  # of breaks less or equal to x. Return the histogram normalised
  # frequency of that bin
  sapply(x, FUN = function(y) {
    if (y < hist_data$breaks[1] |
          y >= hist_data$breaks[length(hist_data$breaks)]) {
      return(0)
    } else {
      return(hist_data$density[sum(hist_data$breaks - y <= 0)])
    }
  })
}

# A density function obtained by linear interpolation
kern_density = function(density_data, x) {
  approxfun(density_data$x, density_data$y, yleft = 0, yright = 0)(x)
}
```


```{r Graphing}
# ggplot2 graphs

overlay_plot = function(data, fun, hist_breaks, dens_ker, dens_bw) {
  ggplot(data = tibble(xx = data), aes(x = xx)) +
    geom_histogram(aes(y = ..density..), breaks = hist_breaks,
                   color = "black", fill = "red", lwd = 1.5, alpha = 0.5) +
    stat_function(fun = fun, color = "black", lwd = 2) +
    stat_density(data = tibble(xx = data), aes(x = xx),
                 kernel = dens_ker, bw = dens_bw,
                 color = "blue", alpha = 0, lwd = 2)
}

max_samples = 250

rdistribs = list(rdistrib_1, rdistrib_2, rdistrib_3, rdistrib_4, rdistrib_5)

ddistribs = list(ddistrib_1, ddistrib_2, ddistrib_3, ddistrib_4, ddistrib_5)

xx = lapply(rdistribs, function(fun) fun(max_samples))

i = 5
overlay_plot(xx[[i]], fun = ddistribs[[i]], hist_breaks = hist(xx[[i]])$breaks,
             dens_ker = "gaussian", dens_bw = "bcv") +
overlay_plot(xx[[1]], fun = ddistribs[[1]], hist_breaks = hist(xx[[1]])$breaks,
             dens_ker = "gaussian", dens_bw = "bcv")

```

```{r old_graphs}
max_samples = 250
# Generate data from the first density
xx = rdistrib_5(max_samples)

plot(density(xx))

normalmix_density = function(x, mix_output) {
  sapply(x, function(y) sum(mix_output$lambda * dnorm(y, mix_output$mu, mix_output$sigma)))
}

mixres = normalmixEM(xx, k = 2)

plot(seq(0, 10, by = 0.1), normalmix_density(seq(0, 10, by = 0.1), mix_output = mixres), type = "l")
```

```{r ISE_calculation}
max_samples = 250

# With histogram density estimation
samples_1 = rdistrib_1(max_samples)

integrate_data = integrate(function(x) {(hist_density(hist_data = hist(samples_1, freq = FALSE, plot = FALSE), x) - ddistrib_1(x))^2},
          lower = -Inf, upper = Inf)

curve((hist_density(hist_data = hist(samples_1, plot = FALSE), x) - ddistrib_1(x)), from = -10, to = 10, n = 1000)


# With kernel density estimation


# With mixture density estimation
```




## Monte Carlo simulation

- ISEs for sample sizes 250, 500, 1000

Two things can be done here:

* Compute a Monte Carlo estimate of the MISE (per method per sample size)
* Plot ISE histograms for each category and as a function of "time" on the same graph

- Discussion
